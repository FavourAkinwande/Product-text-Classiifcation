{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f477c8",
   "metadata": {},
   "source": [
    "# Logistic Regression for Product Title Classification\n",
    "## Comparative Analysis with Multiple Word Embeddings\n",
    "\n",
    "**Author:** David CYUBAHIRO \n",
    "**Date:** February 7, 2026  \n",
    "**Model:** Logistic Regression  \n",
    "**Embeddings:** TF-IDF, Word2Vec (Skip-gram), Word2Vec (CBOW), FastText\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "This notebook implements and evaluates Logistic Regression classifiers using four different word embedding techniques. The goal is to compare performance across embeddings and provide insights for the team's academic report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283f263",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc6a0c",
   "metadata": {},
   "source": [
    "## 0. Install Required Packages\n",
    "\n",
    "Run this cell first if you don't have the required libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eca2666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (2.2.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (1.7.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (4.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\cyuba\\appdata\\roaming\\python\\python313\\site-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn gensim matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d72ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.2.5\n",
      "Gensim version: 4.4.0\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "# Word Embeddings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Gensim version: {gensim.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d9b851",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f3d5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration completed!\n",
      "Output directory: ../results\\LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# Directory paths\n",
    "RESULTS_BASE = \"../results\"\n",
    "EDA_DIR = os.path.join(RESULTS_BASE, \"EDA\")\n",
    "OUTPUT_DIR = os.path.join(RESULTS_BASE, \"LogisticRegression\")\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
    "GRAPHS_DIR = os.path.join(OUTPUT_DIR, \"visual_graphs\")\n",
    "\n",
    "# Data file paths (try multiple locations)\n",
    "TRAIN_CSV_PATHS = [\n",
    "    os.path.join(RESULTS_BASE, \"train.csv\"),\n",
    "    os.path.join(EDA_DIR, \"train.csv\"),\n",
    "]\n",
    "TEST_CSV_PATHS = [\n",
    "    os.path.join(RESULTS_BASE, \"test.csv\"),\n",
    "    os.path.join(EDA_DIR, \"test.csv\"),\n",
    "]\n",
    "\n",
    "# Column names\n",
    "TEXT_COL = \"clean_text\"\n",
    "LABEL_COL = \"label_id\"\n",
    "CATEGORY_COL = \"category_name\"\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Model parameters\n",
    "EMBEDDING_DIM = 100\n",
    "MIN_WORD_COUNT = 2\n",
    "WINDOW_SIZE = 5\n",
    "WORKERS = 4\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(GRAPHS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration completed!\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c837ec",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5ec15",
   "metadata": {},
   "source": [
    "### Option: Download Dataset and Run Preprocessing\n",
    "\n",
    "**If you don't have the data files yet:**\n",
    "\n",
    "1. **Download from Kaggle**: https://www.kaggle.com/datasets/asaniczka/product-titles-text-classification\n",
    "   - Download `titles_to_categories.csv`\n",
    "   - Place it in `../data/titles_to_categories.csv`\n",
    "\n",
    "2. **Run preprocessing**:\n",
    "   ```python\n",
    "   # Uncomment and run if you need to generate train/test splits\n",
    "   # !cd ../src && python preprocessing.py\n",
    "   ```\n",
    "\n",
    "3. **OR ask your teammate** for `train.csv` and `test.csv` files and place them in `../results/`\n",
    "\n",
    "After getting the files, restart from the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "241b2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úó Data files NOT found.\n",
      "\n",
      "You need to either:\n",
      "1. Ask your teammate for train.csv and test.csv\n",
      "2. Download raw data from Kaggle and run preprocessing\n",
      "\n",
      "See instructions in the cell above ‚Üë\n"
     ]
    }
   ],
   "source": [
    "# Quick check: Do you have the data files?\n",
    "import os\n",
    "\n",
    "data_files_exist = (\n",
    "    os.path.exists(\"../results/train.csv\") and \n",
    "    os.path.exists(\"../results/test.csv\")\n",
    ")\n",
    "\n",
    "if data_files_exist:\n",
    "    print(\"‚úì Data files found! You can proceed with the experiments.\")\n",
    "else:\n",
    "    print(\"‚úó Data files NOT found.\")\n",
    "    print(\"\\nYou need to either:\")\n",
    "    print(\"1. Ask your teammate for train.csv and test.csv\")\n",
    "    print(\"2. Download raw data from Kaggle and run preprocessing\")\n",
    "    print(\"\\nSee instructions in the cell above ‚Üë\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION: Run preprocessing if you have the raw data\n",
    "# Uncomment the lines below if you've downloaded titles_to_categories.csv to ../data/\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "# \n",
    "# print(\"Running preprocessing script...\")\n",
    "# result = subprocess.run(\n",
    "#     [sys.executable, \"../src/preprocessing.py\"],\n",
    "#     capture_output=True,\n",
    "#     text=True\n",
    "# )\n",
    "# \n",
    "# if result.returncode == 0:\n",
    "#     print(\"‚úì Preprocessing completed successfully!\")\n",
    "#     print(result.stdout)\n",
    "# else:\n",
    "#     print(\"‚úó Preprocessing failed:\")\n",
    "#     print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d233446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚ö†Ô∏è  DATA FILES NOT FOUND\n",
      "======================================================================\n",
      "\n",
      "The preprocessed data files (train.csv, test.csv) are missing.\n",
      "\n",
      "üìã QUICK FIX - Choose ONE option:\n",
      "\n",
      "‚úÖ Option 1 (FASTEST - Recommended):\n",
      "   Ask your teammate who ran the GRU experiments for:\n",
      "   ‚Ä¢ train.csv\n",
      "   ‚Ä¢ test.csv\n",
      "   Place them in: C:\\Users\\cyuba\\Documents\\LEARN\\ALU\\Product\\Product-text-Classiifcation\\results\\\n",
      "\n",
      " Option 2 (If you have the raw dataset):\n",
      "   1. Download from Kaggle: https://www.kaggle.com/datasets/asaniczka/product-titles-text-classification\n",
      "   2. Place titles_to_categories.csv in ../data/\n",
      "   3. Uncomment and run cell 10 above to generate train/test splits\n",
      "\n",
      "After getting the files, restart the kernel and run all cells.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def find_data_file(paths):\n",
    "    \"\"\"Find the first existing file from a list of paths.\"\"\"\n",
    "    for path in paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "# Load training data\n",
    "train_path = find_data_file(TRAIN_CSV_PATHS)\n",
    "test_path = find_data_file(TEST_CSV_PATHS)\n",
    "\n",
    "if train_path is None or test_path is None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  DATA FILES NOT FOUND\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nThe preprocessed data files (train.csv, test.csv) are missing.\")\n",
    "    print(\"\\nüìã QUICK FIX - Choose ONE option:\\n\")\n",
    "    \n",
    "    print(\"‚úÖ Option 1 (FASTEST - Recommended):\")\n",
    "    print(\"   Ask your teammate who ran the GRU experiments for:\")\n",
    "    print(\"   ‚Ä¢ train.csv\")\n",
    "    print(\"   ‚Ä¢ test.csv\")\n",
    "    print(\"   Place them in: C:\\\\Users\\\\cyuba\\\\Documents\\\\LEARN\\\\ALU\\\\Product\\\\Product-text-Classiifcation\\\\results\\\\\")\n",
    "    print()\n",
    "    \n",
    "    print(\" Option 2 (If you have the raw dataset):\")\n",
    "    print(\"   1. Download from Kaggle: https://www.kaggle.com/datasets/asaniczka/product-titles-text-classification\")\n",
    "    print(\"   2. Place titles_to_categories.csv in ../data/\")\n",
    "    print(\"   3. Uncomment and run cell 10 above to generate train/test splits\")\n",
    "    print()\n",
    "    \n",
    "    print(\"After getting the files, restart the kernel and run all cells.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Set flags so later cells can check\n",
    "    DATA_FILES_AVAILABLE = False\n",
    "    train_df = None\n",
    "    test_df = None\n",
    "else:\n",
    "    print(f\"‚úì Found training data: {train_path}\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    \n",
    "    print(f\"‚úì Found test data: {test_path}\")\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"‚úÖ DATA LOADED SUCCESSFULLY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")\n",
    "    print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "    print(f\"Number of classes: {train_df[LABEL_COL].nunique()}\")\n",
    "    \n",
    "    DATA_FILES_AVAILABLE = True\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample data:\")\n",
    "    display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56035cb6",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3e8ecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Cannot proceed - data files are missing!\n",
      "Please follow the instructions in the previous cells to get the data.\n",
      "After getting train.csv and test.csv, restart the kernel and run all cells.\n"
     ]
    }
   ],
   "source": [
    "# Check if data is available before proceeding\n",
    "if train_df is None or test_df is None:\n",
    "    print(\"‚ö†Ô∏è  Cannot proceed - data files are missing!\")\n",
    "    print(\"Please follow the instructions in the previous cells to get the data.\")\n",
    "    print(\"After getting train.csv and test.csv, restart the kernel and run all cells.\")\n",
    "else:\n",
    "    # Extract features and labels\n",
    "    X_train_text = train_df[TEXT_COL].values\n",
    "    X_test_text = test_df[TEXT_COL].values\n",
    "    y_train = train_df[LABEL_COL].values\n",
    "    y_test = test_df[LABEL_COL].values\n",
    "    \n",
    "    print(f\"‚úì Data preparation complete!\")\n",
    "    print(f\"Training samples: {len(X_train_text):,}\")\n",
    "    print(f\"Test samples: {len(X_test_text):,}\")\n",
    "    print(f\"Label range: {y_train.min()} to {y_train.max()}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    class_dist = train_df[LABEL_COL].value_counts().sort_index()\n",
    "    print(f\"\\nClass distribution (first 10):\")\n",
    "    print(class_dist.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d447d3d",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a459d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"Simple tokenization by splitting on whitespace.\"\"\"\n",
    "    return str(text).split()\n",
    "\n",
    "def save_classification_report(y_true, y_pred, embedding_name):\n",
    "    \"\"\"Save detailed classification report to file.\"\"\"\n",
    "    report = classification_report(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    report_path = os.path.join(OUTPUT_DIR, f\"classification_report_{embedding_name}.txt\")\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"Saved classification report to: {report_path}\")\n",
    "    return report\n",
    "\n",
    "def save_model_and_embedder(model, embedder, embedding_name):\n",
    "    \"\"\"Save trained model and embedding object.\"\"\"\n",
    "    model_path = os.path.join(MODELS_DIR, f\"logreg_{embedding_name}.pkl\")\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    embedder_path = os.path.join(MODELS_DIR, f\"embedder_{embedding_name}.pkl\")\n",
    "    with open(embedder_path, 'wb') as f:\n",
    "        pickle.dump(embedder, f)\n",
    "    \n",
    "    print(f\"Saved model to: {model_path}\")\n",
    "    print(f\"Saved embedder to: {embedder_path}\")\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, embedding_name, top_n=20):\n",
    "    \"\"\"Plot confusion matrix for top N classes.\"\"\"\n",
    "    # Get top N most frequent classes\n",
    "    top_classes = pd.Series(y_true).value_counts().head(top_n).index.tolist()\n",
    "    \n",
    "    # Filter predictions for top classes\n",
    "    mask = np.isin(y_true, top_classes)\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=top_classes)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n",
    "                xticklabels=top_classes, yticklabels=top_classes)\n",
    "    plt.title(f'Confusion Matrix - Logistic Regression ({embedding_name})\\nTop {top_n} Classes')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(GRAPHS_DIR, f'confusion_matrix_{embedding_name}.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved confusion matrix to: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c3cd8",
   "metadata": {},
   "source": [
    "## 6. Experiment 1: TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80348ac",
   "metadata": {},
   "source": [
    "### 6.1 Create TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf012979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  ERROR: Training data not available!\n",
      "Please run the data loading cells above first.\n",
      "Make sure train.csv and test.csv are in ../results/\n"
     ]
    }
   ],
   "source": [
    "# Check if data is available\n",
    "if 'X_train_text' not in globals() or X_train_text is None:\n",
    "    print(\"‚ö†Ô∏è  ERROR: Training data not available!\")\n",
    "    print(\"Please run the data loading cells above first.\")\n",
    "    print(\"Make sure train.csv and test.csv are in ../results/\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXPERIMENT 1: TF-IDF + Logistic Regression\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),  # unigrams and bigrams\n",
    "        min_df=2,\n",
    "        sublinear_tf=True,\n",
    "    )\n",
    "    \n",
    "    # Transform texts\n",
    "    print(\"\\nCreating TF-IDF features...\")\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "    \n",
    "    print(f\"TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "    print(f\"Train shape: {X_train_tfidf.shape}\")\n",
    "    print(f\"Test shape: {X_test_tfidf.shape}\")\n",
    "    print(f\"Sparsity: {(1.0 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3d317",
   "metadata": {},
   "source": [
    "### 6.2 Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeaffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TF-IDF features are available\n",
    "if 'X_train_tfidf' not in globals() or X_train_tfidf is None:\n",
    "    print(\"‚ö†Ô∏è  ERROR: TF-IDF features not available!\")\n",
    "    print(\"Please run the TF-IDF feature creation cell above first.\")\n",
    "    print(\"Make sure the data files are loaded successfully.\")\n",
    "else:\n",
    "    # Define parameter grid\n",
    "    param_grid_tfidf = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs'],\n",
    "        'max_iter': [500],\n",
    "        'class_weight': ['balanced'],\n",
    "    }\n",
    "    \n",
    "    print(\"Hyperparameter tuning for TF-IDF...\")\n",
    "    print(f\"Parameter grid: {param_grid_tfidf}\")\n",
    "    \n",
    "    # GridSearchCV\n",
    "    lr_tfidf_grid = GridSearchCV(\n",
    "        LogisticRegression(random_state=RANDOM_SEED),\n",
    "        param_grid_tfidf,\n",
    "        cv=3,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "    )\n",
    "    \n",
    "    lr_tfidf_grid.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    print(f\"\\nBest parameters: {lr_tfidf_grid.best_params_}\")\n",
    "    print(f\"Best CV F1-macro: {lr_tfidf_grid.best_score_:.4f}\")\n",
    "    \n",
    "    # Get best model\n",
    "    lr_tfidf = lr_tfidf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01461fd2",
   "metadata": {},
   "source": [
    "### 6.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1eca426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  ERROR: TF-IDF model not trained!\n",
      "Please run the hyperparameter tuning cell above first.\n",
      "Make sure all previous cells executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if model was trained\n",
    "if 'lr_tfidf' not in globals() or lr_tfidf is None:\n",
    "    print(\"‚ö†Ô∏è  ERROR: TF-IDF model not trained!\")\n",
    "    print(\"Please run the hyperparameter tuning cell above first.\")\n",
    "    print(\"Make sure all previous cells executed successfully.\")\n",
    "else:\n",
    "    # Predictions\n",
    "    y_pred_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "    \n",
    "    # Metrics\n",
    "    tfidf_accuracy = accuracy_score(y_test, y_pred_tfidf)\n",
    "    tfidf_macro_f1 = f1_score(y_test, y_pred_tfidf, average='macro')\n",
    "    tfidf_weighted_f1 = f1_score(y_test, y_pred_tfidf, average='weighted')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TF-IDF Results:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Test Accuracy:    {tfidf_accuracy:.4f}\")\n",
    "    print(f\"Macro F1:         {tfidf_macro_f1:.4f}\")\n",
    "    print(f\"Weighted F1:      {tfidf_weighted_f1:.4f}\")\n",
    "    \n",
    "    # Save detailed report\n",
    "    tfidf_report = save_classification_report(y_test, y_pred_tfidf, \"tfidf\")\n",
    "    \n",
    "    # Save model\n",
    "    save_model_and_embedder(lr_tfidf, tfidf_vectorizer, \"tfidf\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred_tfidf, \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d4ea9",
   "metadata": {},
   "source": [
    "## 7. Experiment 2: Word2Vec Skip-gram + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d7baf",
   "metadata": {},
   "source": [
    "### 7.1 Train Word2Vec Skip-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2: Word2Vec Skip-gram + Logistic Regression\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Tokenize texts\n",
    "print(\"\\nTokenizing texts...\")\n",
    "train_sentences = [tokenize_text(text) for text in X_train_text]\n",
    "\n",
    "# Train Word2Vec Skip-gram\n",
    "print(\"Training Word2Vec Skip-gram model...\")\n",
    "w2v_skipgram = Word2Vec(\n",
    "    sentences=train_sentences,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=WINDOW_SIZE,\n",
    "    min_count=MIN_WORD_COUNT,\n",
    "    sg=1,  # Skip-gram\n",
    "    workers=WORKERS,\n",
    "    seed=RANDOM_SEED,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(w2v_skipgram.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fa61f",
   "metadata": {},
   "source": [
    "### 7.2 Convert Texts to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word2vec_vector(text, w2v_model):\n",
    "    \"\"\"Convert text to averaged Word2Vec vector.\"\"\"\n",
    "    tokens = tokenize_text(text)\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(\"Converting texts to Skip-gram vectors...\")\n",
    "X_train_skipgram = np.array([text_to_word2vec_vector(text, w2v_skipgram) for text in X_train_text])\n",
    "X_test_skipgram = np.array([text_to_word2vec_vector(text, w2v_skipgram) for text in X_test_text])\n",
    "\n",
    "print(f\"Train shape: {X_train_skipgram.shape}\")\n",
    "print(f\"Test shape: {X_test_skipgram.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcabf1",
   "metadata": {},
   "source": [
    "### 7.3 Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7eafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid_w2v = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [500],\n",
    "    'class_weight': ['balanced'],\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter tuning for Skip-gram...\")\n",
    "lr_skipgram_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_SEED),\n",
    "    param_grid_w2v,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "lr_skipgram_grid.fit(X_train_skipgram, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {lr_skipgram_grid.best_params_}\")\n",
    "print(f\"Best CV F1-macro: {lr_skipgram_grid.best_score_:.4f}\")\n",
    "\n",
    "lr_skipgram = lr_skipgram_grid.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "y_pred_skipgram = lr_skipgram.predict(X_test_skipgram)\n",
    "\n",
    "skipgram_accuracy = accuracy_score(y_test, y_pred_skipgram)\n",
    "skipgram_macro_f1 = f1_score(y_test, y_pred_skipgram, average='macro')\n",
    "skipgram_weighted_f1 = f1_score(y_test, y_pred_skipgram, average='weighted')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Skip-gram Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:    {skipgram_accuracy:.4f}\")\n",
    "print(f\"Macro F1:         {skipgram_macro_f1:.4f}\")\n",
    "print(f\"Weighted F1:      {skipgram_weighted_f1:.4f}\")\n",
    "\n",
    "# Save\n",
    "save_classification_report(y_test, y_pred_skipgram, \"skipgram\")\n",
    "save_model_and_embedder(lr_skipgram, w2v_skipgram, \"skipgram\")\n",
    "plot_confusion_matrix(y_test, y_pred_skipgram, \"skipgram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82db400",
   "metadata": {},
   "source": [
    "## 8. Experiment 3: Word2Vec CBOW + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f3c76",
   "metadata": {},
   "source": [
    "### 8.1 Train Word2Vec CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830451d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 3: Word2Vec CBOW + Logistic Regression\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Word2Vec CBOW\n",
    "print(\"Training Word2Vec CBOW model...\")\n",
    "w2v_cbow = Word2Vec(\n",
    "    sentences=train_sentences,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=WINDOW_SIZE,\n",
    "    min_count=MIN_WORD_COUNT,\n",
    "    sg=0,  # CBOW\n",
    "    workers=WORKERS,\n",
    "    seed=RANDOM_SEED,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(w2v_cbow.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b3f20",
   "metadata": {},
   "source": [
    "### 8.2 Convert Texts and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting texts to CBOW vectors...\")\n",
    "X_train_cbow = np.array([text_to_word2vec_vector(text, w2v_cbow) for text in X_train_text])\n",
    "X_test_cbow = np.array([text_to_word2vec_vector(text, w2v_cbow) for text in X_test_text])\n",
    "\n",
    "print(f\"Train shape: {X_train_cbow.shape}\")\n",
    "print(f\"Test shape: {X_test_cbow.shape}\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "print(\"\\nHyperparameter tuning for CBOW...\")\n",
    "lr_cbow_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_SEED),\n",
    "    param_grid_w2v,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "lr_cbow_grid.fit(X_train_cbow, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {lr_cbow_grid.best_params_}\")\n",
    "print(f\"Best CV F1-macro: {lr_cbow_grid.best_score_:.4f}\")\n",
    "\n",
    "lr_cbow = lr_cbow_grid.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "y_pred_cbow = lr_cbow.predict(X_test_cbow)\n",
    "\n",
    "cbow_accuracy = accuracy_score(y_test, y_pred_cbow)\n",
    "cbow_macro_f1 = f1_score(y_test, y_pred_cbow, average='macro')\n",
    "cbow_weighted_f1 = f1_score(y_test, y_pred_cbow, average='weighted')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CBOW Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:    {cbow_accuracy:.4f}\")\n",
    "print(f\"Macro F1:         {cbow_macro_f1:.4f}\")\n",
    "print(f\"Weighted F1:      {cbow_weighted_f1:.4f}\")\n",
    "\n",
    "# Save\n",
    "save_classification_report(y_test, y_pred_cbow, \"cbow\")\n",
    "save_model_and_embedder(lr_cbow, w2v_cbow, \"cbow\")\n",
    "plot_confusion_matrix(y_test, y_pred_cbow, \"cbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceafb28",
   "metadata": {},
   "source": [
    "## 9. Experiment 4: FastText + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae7205",
   "metadata": {},
   "source": [
    "### 9.1 Train FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 4: FastText + Logistic Regression\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train FastText\n",
    "print(\"Training FastText model...\")\n",
    "fasttext_model = FastText(\n",
    "    sentences=train_sentences,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=WINDOW_SIZE,\n",
    "    min_count=MIN_WORD_COUNT,\n",
    "    workers=WORKERS,\n",
    "    seed=RANDOM_SEED,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(fasttext_model.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27df52",
   "metadata": {},
   "source": [
    "### 9.2 Convert Texts and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_fasttext_vector(text, ft_model):\n",
    "    \"\"\"Convert text to averaged FastText vector.\"\"\"\n",
    "    tokens = tokenize_text(text)\n",
    "    vectors = [ft_model.wv[word] for word in tokens if word in ft_model.wv]\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(\"Converting texts to FastText vectors...\")\n",
    "X_train_fasttext = np.array([text_to_fasttext_vector(text, fasttext_model) for text in X_train_text])\n",
    "X_test_fasttext = np.array([text_to_fasttext_vector(text, fasttext_model) for text in X_test_text])\n",
    "\n",
    "print(f\"Train shape: {X_train_fasttext.shape}\")\n",
    "print(f\"Test shape: {X_test_fasttext.shape}\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "print(\"\\nHyperparameter tuning for FastText...\")\n",
    "lr_fasttext_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_SEED),\n",
    "    param_grid_w2v,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "lr_fasttext_grid.fit(X_train_fasttext, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {lr_fasttext_grid.best_params_}\")\n",
    "print(f\"Best CV F1-macro: {lr_fasttext_grid.best_score_:.4f}\")\n",
    "\n",
    "lr_fasttext = lr_fasttext_grid.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "y_pred_fasttext = lr_fasttext.predict(X_test_fasttext)\n",
    "\n",
    "fasttext_accuracy = accuracy_score(y_test, y_pred_fasttext)\n",
    "fasttext_macro_f1 = f1_score(y_test, y_pred_fasttext, average='macro')\n",
    "fasttext_weighted_f1 = f1_score(y_test, y_pred_fasttext, average='weighted')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FastText Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:    {fasttext_accuracy:.4f}\")\n",
    "print(f\"Macro F1:         {fasttext_macro_f1:.4f}\")\n",
    "print(f\"Weighted F1:      {fasttext_weighted_f1:.4f}\")\n",
    "\n",
    "# Save\n",
    "save_classification_report(y_test, y_pred_fasttext, \"fasttext\")\n",
    "save_model_and_embedder(lr_fasttext, fasttext_model, \"fasttext\")\n",
    "plot_confusion_matrix(y_test, y_pred_fasttext, \"fasttext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d625ac5",
   "metadata": {},
   "source": [
    "## 10. Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995732e9",
   "metadata": {},
   "source": [
    "### 10.1 Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Embedding': ['TF-IDF', 'Skip-gram', 'CBOW', 'FastText'],\n",
    "    'Test Accuracy': [tfidf_accuracy, skipgram_accuracy, cbow_accuracy, fasttext_accuracy],\n",
    "    'Macro F1': [tfidf_macro_f1, skipgram_macro_f1, cbow_macro_f1, fasttext_macro_f1],\n",
    "    'Weighted F1': [tfidf_weighted_f1, skipgram_weighted_f1, cbow_weighted_f1, fasttext_weighted_f1],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON - LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "print(results_comparison.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "comparison_path = os.path.join(OUTPUT_DIR, 'model_comparison_results.csv')\n",
    "results_comparison.to_csv(comparison_path, index=False)\n",
    "print(f\"\\nSaved comparison to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344fb96",
   "metadata": {},
   "source": [
    "### 10.2 Visualization - Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison bar plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['Test Accuracy', 'Macro F1', 'Weighted F1']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    bars = ax.bar(results_comparison['Embedding'], results_comparison[metric], color=colors)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=10)\n",
    "    ax.set_xlabel('Embedding Type', fontsize=10)\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Logistic Regression Performance Across Embeddings', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(GRAPHS_DIR, 'model_comparison.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved comparison plot to: {save_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767790a",
   "metadata": {},
   "source": [
    "### 10.3 Best Model Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing embedding\n",
    "best_accuracy_idx = results_comparison['Test Accuracy'].idxmax()\n",
    "best_macro_f1_idx = results_comparison['Macro F1'].idxmax()\n",
    "best_weighted_f1_idx = results_comparison['Weighted F1'].idxmax()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST PERFORMING EMBEDDINGS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Accuracy:    {results_comparison.loc[best_accuracy_idx, 'Embedding']} \"\n",
    "      f\"({results_comparison.loc[best_accuracy_idx, 'Test Accuracy']:.4f})\")\n",
    "print(f\"Best Macro F1:    {results_comparison.loc[best_macro_f1_idx, 'Embedding']} \"\n",
    "      f\"({results_comparison.loc[best_macro_f1_idx, 'Macro F1']:.4f})\")\n",
    "print(f\"Best Weighted F1: {results_comparison.loc[best_weighted_f1_idx, 'Embedding']} \"\n",
    "      f\"({results_comparison.loc[best_weighted_f1_idx, 'Weighted F1']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b017c",
   "metadata": {},
   "source": [
    "### 10.4 Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4acd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get per-class F1 scores for all embeddings\n",
    "def get_per_class_f1(y_true, y_pred):\n",
    "    _, _, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "    return f1\n",
    "\n",
    "f1_tfidf = get_per_class_f1(y_test, y_pred_tfidf)\n",
    "f1_skipgram = get_per_class_f1(y_test, y_pred_skipgram)\n",
    "f1_cbow = get_per_class_f1(y_test, y_pred_cbow)\n",
    "f1_fasttext = get_per_class_f1(y_test, y_pred_fasttext)\n",
    "\n",
    "# Create dataframe\n",
    "per_class_df = pd.DataFrame({\n",
    "    'Class ID': range(len(f1_tfidf)),\n",
    "    'TF-IDF F1': f1_tfidf,\n",
    "    'Skip-gram F1': f1_skipgram,\n",
    "    'CBOW F1': f1_cbow,\n",
    "    'FastText F1': f1_fasttext,\n",
    "})\n",
    "\n",
    "print(\"\\nPer-class F1 scores (first 10 classes):\")\n",
    "print(per_class_df.head(10).to_string(index=False))\n",
    "\n",
    "# Find classes where embeddings differ most\n",
    "per_class_df['Max Diff'] = per_class_df[['TF-IDF F1', 'Skip-gram F1', 'CBOW F1', 'FastText F1']].max(axis=1) - \\\n",
    "                            per_class_df[['TF-IDF F1', 'Skip-gram F1', 'CBOW F1', 'FastText F1']].min(axis=1)\n",
    "\n",
    "print(\"\\nClasses with largest performance differences:\")\n",
    "print(per_class_df.nlargest(10, 'Max Diff')[['Class ID', 'TF-IDF F1', 'Skip-gram F1', 'CBOW F1', 'FastText F1', 'Max Diff']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928a41c",
   "metadata": {},
   "source": [
    "## 11. Key Insights and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9768984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Total Training Samples: {len(y_train):,}\n",
    "Total Test Samples:     {len(y_test):,}\n",
    "Number of Classes:      {len(np.unique(y_train))}\n",
    "\n",
    "Embeddings Tested:\n",
    "1. TF-IDF (10,000 features, unigrams + bigrams)\n",
    "2. Word2Vec Skip-gram ({EMBEDDING_DIM}d, window={WINDOW_SIZE})\n",
    "3. Word2Vec CBOW ({EMBEDDING_DIM}d, window={WINDOW_SIZE})\n",
    "4. FastText ({EMBEDDING_DIM}d, window={WINDOW_SIZE})\n",
    "\n",
    "Model: Logistic Regression\n",
    "- Hyperparameter tuning: GridSearchCV (3-fold CV)\n",
    "- Optimization metric: F1-macro\n",
    "- Class weighting: Balanced\n",
    "\n",
    "All results saved to: {OUTPUT_DIR}\n",
    "\"\"\")\n",
    "\n",
    "# Create final summary JSON\n",
    "summary = {\n",
    "    \"experiment_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_type\": \"Logistic Regression\",\n",
    "    \"dataset\": {\n",
    "        \"train_samples\": int(len(y_train)),\n",
    "        \"test_samples\": int(len(y_test)),\n",
    "        \"num_classes\": int(len(np.unique(y_train))),\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tfidf\": {\n",
    "            \"accuracy\": float(tfidf_accuracy),\n",
    "            \"macro_f1\": float(tfidf_macro_f1),\n",
    "            \"weighted_f1\": float(tfidf_weighted_f1),\n",
    "        },\n",
    "        \"skipgram\": {\n",
    "            \"accuracy\": float(skipgram_accuracy),\n",
    "            \"macro_f1\": float(skipgram_macro_f1),\n",
    "            \"weighted_f1\": float(skipgram_weighted_f1),\n",
    "        },\n",
    "        \"cbow\": {\n",
    "            \"accuracy\": float(cbow_accuracy),\n",
    "            \"macro_f1\": float(cbow_macro_f1),\n",
    "            \"weighted_f1\": float(cbow_weighted_f1),\n",
    "        },\n",
    "        \"fasttext\": {\n",
    "            \"accuracy\": float(fasttext_accuracy),\n",
    "            \"macro_f1\": float(fasttext_macro_f1),\n",
    "            \"weighted_f1\": float(fasttext_weighted_f1),\n",
    "        },\n",
    "    },\n",
    "    \"best_embedding\": {\n",
    "        \"by_accuracy\": results_comparison.loc[best_accuracy_idx, 'Embedding'],\n",
    "        \"by_macro_f1\": results_comparison.loc[best_macro_f1_idx, 'Embedding'],\n",
    "        \"by_weighted_f1\": results_comparison.loc[best_weighted_f1_idx, 'Embedding'],\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(OUTPUT_DIR, 'experiment_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved experiment summary to: {summary_path}\")\n",
    "print(\"\\n‚úì ALL EXPERIMENTS COMPLETED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701eea58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps for Team Report\n",
    "\n",
    "1. **Compare with GRU results** in `results/GRU/`\n",
    "2. **Analyze why different embeddings perform better/worse** with Logistic Regression vs. GRU\n",
    "3. **Document hyperparameters** used in each experiment\n",
    "4. **Create consolidated visualizations** comparing all team members' models\n",
    "5. **Write insights section** explaining the trade-offs between model complexity and embedding choice\n",
    "\n",
    "**Key Questions to Address:**\n",
    "- How does Logistic Regression compare to GRU for this task?\n",
    "- Which embedding works best with linear models vs. neural models?\n",
    "- What is the computational cost vs. performance trade-off?\n",
    "- How does class imbalance affect different model-embedding combinations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
